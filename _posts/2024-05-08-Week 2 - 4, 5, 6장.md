---
quickshare-date: 2024-05-08 19:13:14
quickshare-url: "https://noteshare.space/note/clvxnvqxy1728701mwyr45gx3c#z8RrUWLg15HWh4+eCMjd+78cDfjaLyF72hIMhF+7338"
---
# 4장 : 가장 훌륭한 예측선

- 딥러닝을 이해하기 위해서 알아야 하는 두 가지
	- **선형 회귀**
	- **로지스틱 회귀**
- 가장 훌륭한 예측선 : 선형 회귀를 의미
## 선형 회귀의 정의

- **독립 변수** : 독립적으로 변할 수 있는 값 (ex: 성적을 변하게 하는 *정보*)
- **종속 변수** : 독립 변수에 따라 종속적으로 변화하는 값 (ex: *성적*)
- **선형 회귀** : 독립 변수를 이용해 종속 변수의 움직임을 예측하고 설명하는 작업

- **단순 선형 회귀** : 하나의 x값으로 y값을 설명할 수 있을 때
- **다중 선형 회귀** : x값이 여러 개 필요할 때

## 가장 훌륭한 예측선이란?

- 만약 주어진 데이터가 선형을 보인다면 -> 이 특징을 잘 나타내는 일차 함수 그래프를 그릴 수 있음
$$y = ax+b$$
- $x$ : 독립 변수, $y$ : 종속 변수 -> $x$값에 따라 $y$값이 변화함.
- but 이를 정확히 예측하기 위해서는 정확한 a, b값이 필요
- 정확한 a, b값을 구해낸다면?
	- <mark style="background: #FFF3A3A6;">주어지지 않은 다른 학생의 성적을 예측</mark>하는 것이 가능!
- 따라서 선형 회귀의 개념을 이해하는 것은 <mark class="hltr-yellow">딥러닝을 이해하는 데 중요한 첫걸음</mark>

## 최소 제곱법

- **최소 제곱법**을 통해 일차 함수의 기울기 a와 b를 바로 구하는 것이 가능.
- 공식은 다음과 같음 :
$$a = \frac{(x-x평균)(y-y평균)의합}{(x-x평균)^2의합}$$
> 분자 : x의 편차의 제곱의 합
> 분모 : x와 y의 편차를 곱해 합한 값 

$$b = y의 평균 - (x의 평균 * 기울기 a)$$
- 이를 통해 만든 직선에 값을 대입함으로써 결과를 예측할 수 있음

## 평균 제곱 오차

- 선을 그을 때 가장 많이 사용하는 방법 : 일단 그리고 조금씩 수정해 나가기
- 이 과정에서 선의 오차를 계산하는 방법이 필요
- 가장 많이 사용되는 오차 평가 방법 : **평균 제곱 오차**
- 만약 오차를 제곱 없이 그대로 더할 시 부호가 서로 다르므로 값이 0이 될 수 있음
	- -> *제곱해서 더해주기!*
- **평균 제곱 오차(MSE)** : 오차의 합을 원소의 개수로 나눈 것
$$
평균 \,제곱 \, 오차(MSE) = \frac{1}{n}\sum(y_i-\hat y_i)^2
$$

# 선형 회귀 모델 : 먼저 긋고 수정하기

- 기울기와 오차 : 적절한 기울기를 찾았을 때 오차가 최소화되는 **이차 함수의 관계**를 가짐.
- **경사 하강법** : 미분 기울기를 이용하여 오차가 최소화되는 최적의 기울기를 찾는 방법

## 경사 하강법의 개요

- 한 지점에서 *기울기가 0일 때* 그 지점의 값이 0이 됨. (2차 함수)
- 이를 위해 다음 과정을 거침
	1. $a_1$에서 미분을 구함
	2. 구한 기울기의 반대 방향 (+면 음의 방향, -면 양의 방향)으로 얼마간 이동시킨 $a_2$에서 미분을 구함
	3. 앞에서 구한 미분 값이 0이 아니라면 1과 2 과정을 반복
- 이와 같이 기울기를 변화시켜 m값을 찾아냄.
- **학습률** : 얼마나 이동시킬지 그 거리를 결정하는 값
- **경사 하강법** : 오차의 변화에 따라 *이차 함수 그래프*를 만들고 적절한 *학습률*을 설정해 *미분 값이 0인 지점*을 구하는 것

## 다중 선형 회귀의 개요

- **다중 선형 회귀** : 단순 선형 회귀에서 변수 개수를 늘린 것
- 다음과 같은 수식으로 표현할 수 있음 : 
$$
y=a_1x_1 + a_2x_2 + b
$$
## 텐서플로에서 실행하는 선형 회귀, 다중 선형 회귀 모델

- $y = ax+b$와 같은 수식 : *가설 함수*, $H(x)$로 표기
- 기울기 a : *가중치*, $w$로 표기
- 절편 b : *편향*, $b$로 표기
- 실제 값과 예측 값 사이의 오차에 대한 식 : *손실 함수*
- 경사 하강법과 같은 최적의 기울기와 절편을 찾기 위해 사용하는 방법 : *옵티마이저

# 로지스틱 회귀 모델 : 참 거짓 판단하기

## 로지스틱 회귀의 정의

- 선형회귀와 마찬가지로 참/거짓 판단을 위해 적절한 선을 그려가는 과정
	- -> 여기서 직선이 아니라 참과 거짓을 구별하는 *S자 형태의 곡선*을 그려주는 것이 차이점!
## 시그모이드 함수

- S자 모양으로 그래프가 그려지는 함수
- 다음과 같은 공식을 가짐 :
$$
y=\frac{1}{1+e^{-(ax+b)}}
$$
- $a$ : 그래프의 *경사도* - 커지면 경사 증가
- $b$ : 그래프의 *좌우 이동* - 커지면 왼쪽으로 이동
- $a$값이 작아지면 *오차 무한대로 증가*, but 커진다고 해서 사라지지는 않음
- $b$값에 따른 오차는 *이차 함수 그래프*와 유사한 형태로 나타남
## 오차 공식

- 경사 하강법 사용, but MSE 사용 불가 
- y값이 0과 1 사이라는 특징을 이용해 오차를 구함 -> **로그 함수**
## 로그 함수

- 실제 값이 1일 때 -> 예측 값이 0일 수록 오차가 커지는 그래프
- 실제 값이 0일 때 -> 예측 값이 1일 수록 오차가 커지는 그래프
- 전자의 식 : $-\log h$
- 후자의 식 : $-\log (1-h)$
- 다음과 같은 방법으로 하나의 식으로 합침
$$
-{y\log h + (1-y)\log (1-h)}
$$
- 실제 값이 1이면 전자의 식만, 0이면 후자의 식만 사용 가능
- 위와 같은 손실 함수 : **교차 엔트로피 오차**
