<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CS Playground</title><meta name=keywords content><meta name=description content="Abstract

Bidirectional Encoder Representations from Transformers (트랜스포머를 통한 양방향 인코더 표현)을 소개
기존 언어 표현 모델과 다르게, BERT는 모든 레이어의 왼쪽과 오른쪽 맥락을 동시에 고려하는 방식으로 라벨링되지 않은 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계.

-> 테스크를 위한 구체적인 구조 수정 없이도 하나의 아웃풋 레이어를 추가하는 것 만으로 다양한 태스크를 위한 SOTA 모델을 생성하는 것이 가능.



Introduction

언어모델 사전 학습이 많은 NLP 태스크에 있어 효과적임이 증명됨.

문장 단계의 태스크 : 문장 간의 연관성을 전체적으로 분석함으로써 예측

자연 언어 추론
parapharsing


토큰 단계의 태스크 : 토큰 수준에서 세밀한 아웃풋을 생성

개체명 인식
질문 답변




사전학습된 언어 표현을 후속 태스크에 적용하기 위한 방법은 두 가지가 있음.

feature-based (ex: ELMo)

추가적 특징으로 사전학습된 표현을 포함하는 태스크 구체적인 구조를 사용.


fine-tuning (ex: OpenAI GPT)

최소한의 태스크 구체적인 파라미터를 도입
모든 사전학습된 파라미터를 파인튜닝함으로써 후속 태스크에 맞춰 학습.




이런 기존의 기술이 특히 fine-tuning 기법에서 사전 학습된 표현의 힘을 제약한다고 주장.

주요 제한 사항 : 현대 언어 모델은 단방향이라는 것.
ex) GPT : 왼쪽에서 오른쪽으로 진행하는 구조를 사용 -> 트랜스포머의 self-attention layer의 이전 토큰만을 접근 가능.
-> 이러한 제약 사항은 문장 수준에 태스크에 있어 최선은 아니고, 양쪽 방향에서의 맥락을 통합하는 것이 중요한 토큰 단위 태스크에서 fine-tuning 기법을 적용하는 것은 위험할 수 있음.


이 paper에서는 BERT를 제안함으로써 파인튜닝 기반 기법을 개선함.

Masked Language Model을 통해 사전학습을 함으로써 단방향성의 제약을 완화.

MLM : 인풋의 토큰을 랜덤으로 가린 뒤 문맥만을 통해 가려진 단어를 유추.
왼쪽과 오른쪽 문맥을 표현이 융합할 수 있도록 함.


다음 문장 예측 태스크를 통해 문장 쌍 표현을 연결해 사전훈련


기여는 다음과 같음.

언어 표현을 위한 양방향 사전 학습의 중요성을 설명.

MLM을 통해 사전학습된 깊은 양방향 표현을 가능하도록 함.
이는 GPT : 사전학습을 위해 단방향 모델을 사용, 그리고 ELMo : 독립적으로 왼쪽-오른쪽, 오른쪽-왼쪽으로 학습된 언어모델을 얕게 연결하기만 함 과 다름.


사전학습된 표현이 태스크를 위한 구조를 설계하기 위한 노력을 줄인다는 것을 보임.

많은 태스크 구체적인 구조보다 더 잘 작동함으로써 BERT가 넓은 범위의 sentence level, token level의 task에 SOTA를 달성함을 보임.


7개의 NLP 태스크에서 SOTA를 보임.



Related Work
Unsupervised Feature-based Approaches

non-neural과 neural에 걸쳐 널리 사용할 수 있는 단어의 표현을 학습하는 연구는 활발히 진행되어 왔음.
사전 훈련된 단어 임베딩 : 현대 NLP 시스템의 가장 필수적인 부분.

좌에서 우로 단어 예측, 또는 좌우 문백을 통해 올바른 단어와 틀린 단어를 구분하는 목표가 사용됨.


이러한 접근이 문장이나 문단 임베딩과 같은 더 덜 세밀한 범위로 일반화됨.

다음과 같은 목표들이 문장 표현 훈련을 위해 사용됨.

다음 문장 후보 순위를 매김.
이전 문장의 표현을 주고 다음 문장 단어를 좌에서 우로 생성
오토인코더로 생성된 목표의 노이즈를 제거




ELMo와 그 이전 연구들 : 단어 임베딩을 다른 차원에서 일반화함.

좌->우, 우->좌 LM을 통해 문맥과 연관된 특징을 추출.
각 토큰의 문맥적 표현은 좌->우, 우->좌 표현이 합쳐진 것.
존재하는 태스크 구체적인 구조와 문맥적인 단어 임베딩을 통합했을 때, 7개의 major NLP task에서 SOTA를 달성함.
그 외 : LSTM을 사용해 좌우 맥락을 모두 고려해 단어 하나를 예측하는 태스크를 통해 문맥적 표현을 학습하는 방법이 제안됨. 특정 단어를 가리고 맞추는 태스크가 문장 새성 모델의 강건함을 개선하는 데 도움이 됨을 보임.



Unsupervised Fine-tuning Approaches

라벨링되지 않은 텍스트에서 단어 임베딩 파라미터만을 사전학습한 작업물이 시초.
최근은 문맥적 토큰 표현을 생성하는 단어, 문서 인코더가 라벨링되지 않은 문장으로부터 사전학습되고, 하위 태스크를 위해 파인튜닝됨.

완전히 처음부터 학습되어야 하는 파라미터의 수가 적다는 장점이 있음.


이 장점 덕분에, GPT는 GLUE 벤치마크를 통해 문장 단위 태스크에 있어 SOTA를 달성함.
좌-우 언어 모델링, 오토 인코더 목적이 이런 모델을 사전 훈련하는 데 있어 사용됨.

Transfer Learning from Supervised Data

대규모 데이터셋을 사용한 지도 학습에서도 효과적인 전이가 가능하다는 것을 확인할 수 있었음.
컴퓨터 비전 연구에서도 이미지넷으로 사전 훈련된 모델을 파인튜닝하는 것이 효과적임을 확인함으로써, 큰 사전학습된 모델로부터 전이학습하는 것이 중요함을 확인할 수 있었음.

BERT
모델 구조

original transformer를 기반으로 한 multi-layer bidirectional Transformer encoder를 사용.
두 가지의 모델을 사용. L : layer number, h : hidden size, a : number of self-attention heads
BERT base : L=12, H=768, A=12, total parameters : 110M (GPT와 비교를 위해 같은 모델 사이즈를 사용)
BERT large : L=24, H=1024, A=16, total parameters=340M

입/출력 표현


다양한 하위 태스크를 처리할 수 있도록 하기 위해 하나의 토큰 시퀀스에 하나의 문장과 문장 두 개를 모호하지 않게 인풋에 담을 수 있도록 함."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/bert-%EC%A0%95%EB%A6%AC/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/bert-%EC%A0%95%EB%A6%AC/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="http://localhost:1313/posts/bert-%EC%A0%95%EB%A6%AC/"><meta property="og:site_name" content="CS Playground"><meta property="og:title" content="CS Playground"><meta property="og:description" content="Abstract Bidirectional Encoder Representations from Transformers (트랜스포머를 통한 양방향 인코더 표현)을 소개 기존 언어 표현 모델과 다르게, BERT는 모든 레이어의 왼쪽과 오른쪽 맥락을 동시에 고려하는 방식으로 라벨링되지 않은 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계. -> 테스크를 위한 구체적인 구조 수정 없이도 하나의 아웃풋 레이어를 추가하는 것 만으로 다양한 태스크를 위한 SOTA 모델을 생성하는 것이 가능. Introduction 언어모델 사전 학습이 많은 NLP 태스크에 있어 효과적임이 증명됨. 문장 단계의 태스크 : 문장 간의 연관성을 전체적으로 분석함으로써 예측 자연 언어 추론 parapharsing 토큰 단계의 태스크 : 토큰 수준에서 세밀한 아웃풋을 생성 개체명 인식 질문 답변 사전학습된 언어 표현을 후속 태스크에 적용하기 위한 방법은 두 가지가 있음. feature-based (ex: ELMo) 추가적 특징으로 사전학습된 표현을 포함하는 태스크 구체적인 구조를 사용. fine-tuning (ex: OpenAI GPT) 최소한의 태스크 구체적인 파라미터를 도입 모든 사전학습된 파라미터를 파인튜닝함으로써 후속 태스크에 맞춰 학습. 이런 기존의 기술이 특히 fine-tuning 기법에서 사전 학습된 표현의 힘을 제약한다고 주장. 주요 제한 사항 : 현대 언어 모델은 단방향이라는 것. ex) GPT : 왼쪽에서 오른쪽으로 진행하는 구조를 사용 -> 트랜스포머의 self-attention layer의 이전 토큰만을 접근 가능. -> 이러한 제약 사항은 문장 수준에 태스크에 있어 최선은 아니고, 양쪽 방향에서의 맥락을 통합하는 것이 중요한 토큰 단위 태스크에서 fine-tuning 기법을 적용하는 것은 위험할 수 있음. 이 paper에서는 BERT를 제안함으로써 파인튜닝 기반 기법을 개선함. Masked Language Model을 통해 사전학습을 함으로써 단방향성의 제약을 완화. MLM : 인풋의 토큰을 랜덤으로 가린 뒤 문맥만을 통해 가려진 단어를 유추. 왼쪽과 오른쪽 문맥을 표현이 융합할 수 있도록 함. 다음 문장 예측 태스크를 통해 문장 쌍 표현을 연결해 사전훈련 기여는 다음과 같음. 언어 표현을 위한 양방향 사전 학습의 중요성을 설명. MLM을 통해 사전학습된 깊은 양방향 표현을 가능하도록 함. 이는 GPT : 사전학습을 위해 단방향 모델을 사용, 그리고 ELMo : 독립적으로 왼쪽-오른쪽, 오른쪽-왼쪽으로 학습된 언어모델을 얕게 연결하기만 함 과 다름. 사전학습된 표현이 태스크를 위한 구조를 설계하기 위한 노력을 줄인다는 것을 보임. 많은 태스크 구체적인 구조보다 더 잘 작동함으로써 BERT가 넓은 범위의 sentence level, token level의 task에 SOTA를 달성함을 보임. 7개의 NLP 태스크에서 SOTA를 보임. Related Work Unsupervised Feature-based Approaches non-neural과 neural에 걸쳐 널리 사용할 수 있는 단어의 표현을 학습하는 연구는 활발히 진행되어 왔음. 사전 훈련된 단어 임베딩 : 현대 NLP 시스템의 가장 필수적인 부분. 좌에서 우로 단어 예측, 또는 좌우 문백을 통해 올바른 단어와 틀린 단어를 구분하는 목표가 사용됨. 이러한 접근이 문장이나 문단 임베딩과 같은 더 덜 세밀한 범위로 일반화됨. 다음과 같은 목표들이 문장 표현 훈련을 위해 사용됨. 다음 문장 후보 순위를 매김. 이전 문장의 표현을 주고 다음 문장 단어를 좌에서 우로 생성 오토인코더로 생성된 목표의 노이즈를 제거 ELMo와 그 이전 연구들 : 단어 임베딩을 다른 차원에서 일반화함. 좌->우, 우->좌 LM을 통해 문맥과 연관된 특징을 추출. 각 토큰의 문맥적 표현은 좌->우, 우->좌 표현이 합쳐진 것. 존재하는 태스크 구체적인 구조와 문맥적인 단어 임베딩을 통합했을 때, 7개의 major NLP task에서 SOTA를 달성함. 그 외 : LSTM을 사용해 좌우 맥락을 모두 고려해 단어 하나를 예측하는 태스크를 통해 문맥적 표현을 학습하는 방법이 제안됨. 특정 단어를 가리고 맞추는 태스크가 문장 새성 모델의 강건함을 개선하는 데 도움이 됨을 보임. Unsupervised Fine-tuning Approaches 라벨링되지 않은 텍스트에서 단어 임베딩 파라미터만을 사전학습한 작업물이 시초. 최근은 문맥적 토큰 표현을 생성하는 단어, 문서 인코더가 라벨링되지 않은 문장으로부터 사전학습되고, 하위 태스크를 위해 파인튜닝됨. 완전히 처음부터 학습되어야 하는 파라미터의 수가 적다는 장점이 있음. 이 장점 덕분에, GPT는 GLUE 벤치마크를 통해 문장 단위 태스크에 있어 SOTA를 달성함. 좌-우 언어 모델링, 오토 인코더 목적이 이런 모델을 사전 훈련하는 데 있어 사용됨. Transfer Learning from Supervised Data 대규모 데이터셋을 사용한 지도 학습에서도 효과적인 전이가 가능하다는 것을 확인할 수 있었음. 컴퓨터 비전 연구에서도 이미지넷으로 사전 훈련된 모델을 파인튜닝하는 것이 효과적임을 확인함으로써, 큰 사전학습된 모델로부터 전이학습하는 것이 중요함을 확인할 수 있었음. BERT 모델 구조 original transformer를 기반으로 한 multi-layer bidirectional Transformer encoder를 사용. 두 가지의 모델을 사용. L : layer number, h : hidden size, a : number of self-attention heads BERT base : L=12, H=768, A=12, total parameters : 110M (GPT와 비교를 위해 같은 모델 사이즈를 사용) BERT large : L=24, H=1024, A=16, total parameters=340M 입/출력 표현 다양한 하위 태스크를 처리할 수 있도록 하기 위해 하나의 토큰 시퀀스에 하나의 문장과 문장 두 개를 모호하지 않게 인풋에 담을 수 있도록 함."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content><meta name=twitter:description content="Abstract

Bidirectional Encoder Representations from Transformers (트랜스포머를 통한 양방향 인코더 표현)을 소개
기존 언어 표현 모델과 다르게, BERT는 모든 레이어의 왼쪽과 오른쪽 맥락을 동시에 고려하는 방식으로 라벨링되지 않은 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계.

-> 테스크를 위한 구체적인 구조 수정 없이도 하나의 아웃풋 레이어를 추가하는 것 만으로 다양한 태스크를 위한 SOTA 모델을 생성하는 것이 가능.



Introduction

언어모델 사전 학습이 많은 NLP 태스크에 있어 효과적임이 증명됨.

문장 단계의 태스크 : 문장 간의 연관성을 전체적으로 분석함으로써 예측

자연 언어 추론
parapharsing


토큰 단계의 태스크 : 토큰 수준에서 세밀한 아웃풋을 생성

개체명 인식
질문 답변




사전학습된 언어 표현을 후속 태스크에 적용하기 위한 방법은 두 가지가 있음.

feature-based (ex: ELMo)

추가적 특징으로 사전학습된 표현을 포함하는 태스크 구체적인 구조를 사용.


fine-tuning (ex: OpenAI GPT)

최소한의 태스크 구체적인 파라미터를 도입
모든 사전학습된 파라미터를 파인튜닝함으로써 후속 태스크에 맞춰 학습.




이런 기존의 기술이 특히 fine-tuning 기법에서 사전 학습된 표현의 힘을 제약한다고 주장.

주요 제한 사항 : 현대 언어 모델은 단방향이라는 것.
ex) GPT : 왼쪽에서 오른쪽으로 진행하는 구조를 사용 -> 트랜스포머의 self-attention layer의 이전 토큰만을 접근 가능.
-> 이러한 제약 사항은 문장 수준에 태스크에 있어 최선은 아니고, 양쪽 방향에서의 맥락을 통합하는 것이 중요한 토큰 단위 태스크에서 fine-tuning 기법을 적용하는 것은 위험할 수 있음.


이 paper에서는 BERT를 제안함으로써 파인튜닝 기반 기법을 개선함.

Masked Language Model을 통해 사전학습을 함으로써 단방향성의 제약을 완화.

MLM : 인풋의 토큰을 랜덤으로 가린 뒤 문맥만을 통해 가려진 단어를 유추.
왼쪽과 오른쪽 문맥을 표현이 융합할 수 있도록 함.


다음 문장 예측 태스크를 통해 문장 쌍 표현을 연결해 사전훈련


기여는 다음과 같음.

언어 표현을 위한 양방향 사전 학습의 중요성을 설명.

MLM을 통해 사전학습된 깊은 양방향 표현을 가능하도록 함.
이는 GPT : 사전학습을 위해 단방향 모델을 사용, 그리고 ELMo : 독립적으로 왼쪽-오른쪽, 오른쪽-왼쪽으로 학습된 언어모델을 얕게 연결하기만 함 과 다름.


사전학습된 표현이 태스크를 위한 구조를 설계하기 위한 노력을 줄인다는 것을 보임.

많은 태스크 구체적인 구조보다 더 잘 작동함으로써 BERT가 넓은 범위의 sentence level, token level의 task에 SOTA를 달성함을 보임.


7개의 NLP 태스크에서 SOTA를 보임.



Related Work
Unsupervised Feature-based Approaches

non-neural과 neural에 걸쳐 널리 사용할 수 있는 단어의 표현을 학습하는 연구는 활발히 진행되어 왔음.
사전 훈련된 단어 임베딩 : 현대 NLP 시스템의 가장 필수적인 부분.

좌에서 우로 단어 예측, 또는 좌우 문백을 통해 올바른 단어와 틀린 단어를 구분하는 목표가 사용됨.


이러한 접근이 문장이나 문단 임베딩과 같은 더 덜 세밀한 범위로 일반화됨.

다음과 같은 목표들이 문장 표현 훈련을 위해 사용됨.

다음 문장 후보 순위를 매김.
이전 문장의 표현을 주고 다음 문장 단어를 좌에서 우로 생성
오토인코더로 생성된 목표의 노이즈를 제거




ELMo와 그 이전 연구들 : 단어 임베딩을 다른 차원에서 일반화함.

좌->우, 우->좌 LM을 통해 문맥과 연관된 특징을 추출.
각 토큰의 문맥적 표현은 좌->우, 우->좌 표현이 합쳐진 것.
존재하는 태스크 구체적인 구조와 문맥적인 단어 임베딩을 통합했을 때, 7개의 major NLP task에서 SOTA를 달성함.
그 외 : LSTM을 사용해 좌우 맥락을 모두 고려해 단어 하나를 예측하는 태스크를 통해 문맥적 표현을 학습하는 방법이 제안됨. 특정 단어를 가리고 맞추는 태스크가 문장 새성 모델의 강건함을 개선하는 데 도움이 됨을 보임.



Unsupervised Fine-tuning Approaches

라벨링되지 않은 텍스트에서 단어 임베딩 파라미터만을 사전학습한 작업물이 시초.
최근은 문맥적 토큰 표현을 생성하는 단어, 문서 인코더가 라벨링되지 않은 문장으로부터 사전학습되고, 하위 태스크를 위해 파인튜닝됨.

완전히 처음부터 학습되어야 하는 파라미터의 수가 적다는 장점이 있음.


이 장점 덕분에, GPT는 GLUE 벤치마크를 통해 문장 단위 태스크에 있어 SOTA를 달성함.
좌-우 언어 모델링, 오토 인코더 목적이 이런 모델을 사전 훈련하는 데 있어 사용됨.

Transfer Learning from Supervised Data

대규모 데이터셋을 사용한 지도 학습에서도 효과적인 전이가 가능하다는 것을 확인할 수 있었음.
컴퓨터 비전 연구에서도 이미지넷으로 사전 훈련된 모델을 파인튜닝하는 것이 효과적임을 확인함으로써, 큰 사전학습된 모델로부터 전이학습하는 것이 중요함을 확인할 수 있었음.

BERT
모델 구조

original transformer를 기반으로 한 multi-layer bidirectional Transformer encoder를 사용.
두 가지의 모델을 사용. L : layer number, h : hidden size, a : number of self-attention heads
BERT base : L=12, H=768, A=12, total parameters : 110M (GPT와 비교를 위해 같은 모델 사이즈를 사용)
BERT large : L=24, H=1024, A=16, total parameters=340M

입/출력 표현


다양한 하위 태스크를 처리할 수 있도록 하기 위해 하나의 토큰 시퀀스에 하나의 문장과 문장 두 개를 모호하지 않게 인풋에 담을 수 있도록 함."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"","item":"http://localhost:1313/posts/bert-%EC%A0%95%EB%A6%AC/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"Abstract Bidirectional Encoder Representations from Transformers (트랜스포머를 통한 양방향 인코더 표현)을 소개 기존 언어 표현 모델과 다르게, BERT는 모든 레이어의 왼쪽과 오른쪽 맥락을 동시에 고려하는 방식으로 라벨링되지 않은 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계. -\u0026gt; 테스크를 위한 구체적인 구조 수정 없이도 하나의 아웃풋 레이어를 추가하는 것 만으로 다양한 태스크를 위한 SOTA 모델을 생성하는 것이 가능. Introduction 언어모델 사전 학습이 많은 NLP 태스크에 있어 효과적임이 증명됨. 문장 단계의 태스크 : 문장 간의 연관성을 전체적으로 분석함으로써 예측 자연 언어 추론 parapharsing 토큰 단계의 태스크 : 토큰 수준에서 세밀한 아웃풋을 생성 개체명 인식 질문 답변 사전학습된 언어 표현을 후속 태스크에 적용하기 위한 방법은 두 가지가 있음. feature-based (ex: ELMo) 추가적 특징으로 사전학습된 표현을 포함하는 태스크 구체적인 구조를 사용. fine-tuning (ex: OpenAI GPT) 최소한의 태스크 구체적인 파라미터를 도입 모든 사전학습된 파라미터를 파인튜닝함으로써 후속 태스크에 맞춰 학습. 이런 기존의 기술이 특히 fine-tuning 기법에서 사전 학습된 표현의 힘을 제약한다고 주장. 주요 제한 사항 : 현대 언어 모델은 단방향이라는 것. ex) GPT : 왼쪽에서 오른쪽으로 진행하는 구조를 사용 -\u0026gt; 트랜스포머의 self-attention layer의 이전 토큰만을 접근 가능. -\u0026gt; 이러한 제약 사항은 문장 수준에 태스크에 있어 최선은 아니고, 양쪽 방향에서의 맥락을 통합하는 것이 중요한 토큰 단위 태스크에서 fine-tuning 기법을 적용하는 것은 위험할 수 있음. 이 paper에서는 BERT를 제안함으로써 파인튜닝 기반 기법을 개선함. Masked Language Model을 통해 사전학습을 함으로써 단방향성의 제약을 완화. MLM : 인풋의 토큰을 랜덤으로 가린 뒤 문맥만을 통해 가려진 단어를 유추. 왼쪽과 오른쪽 문맥을 표현이 융합할 수 있도록 함. 다음 문장 예측 태스크를 통해 문장 쌍 표현을 연결해 사전훈련 기여는 다음과 같음. 언어 표현을 위한 양방향 사전 학습의 중요성을 설명. MLM을 통해 사전학습된 깊은 양방향 표현을 가능하도록 함. 이는 GPT : 사전학습을 위해 단방향 모델을 사용, 그리고 ELMo : 독립적으로 왼쪽-오른쪽, 오른쪽-왼쪽으로 학습된 언어모델을 얕게 연결하기만 함 과 다름. 사전학습된 표현이 태스크를 위한 구조를 설계하기 위한 노력을 줄인다는 것을 보임. 많은 태스크 구체적인 구조보다 더 잘 작동함으로써 BERT가 넓은 범위의 sentence level, token level의 task에 SOTA를 달성함을 보임. 7개의 NLP 태스크에서 SOTA를 보임. Related Work Unsupervised Feature-based Approaches non-neural과 neural에 걸쳐 널리 사용할 수 있는 단어의 표현을 학습하는 연구는 활발히 진행되어 왔음. 사전 훈련된 단어 임베딩 : 현대 NLP 시스템의 가장 필수적인 부분. 좌에서 우로 단어 예측, 또는 좌우 문백을 통해 올바른 단어와 틀린 단어를 구분하는 목표가 사용됨. 이러한 접근이 문장이나 문단 임베딩과 같은 더 덜 세밀한 범위로 일반화됨. 다음과 같은 목표들이 문장 표현 훈련을 위해 사용됨. 다음 문장 후보 순위를 매김. 이전 문장의 표현을 주고 다음 문장 단어를 좌에서 우로 생성 오토인코더로 생성된 목표의 노이즈를 제거 ELMo와 그 이전 연구들 : 단어 임베딩을 다른 차원에서 일반화함. 좌-\u0026gt;우, 우-\u0026gt;좌 LM을 통해 문맥과 연관된 특징을 추출. 각 토큰의 문맥적 표현은 좌-\u0026gt;우, 우-\u0026gt;좌 표현이 합쳐진 것. 존재하는 태스크 구체적인 구조와 문맥적인 단어 임베딩을 통합했을 때, 7개의 major NLP task에서 SOTA를 달성함. 그 외 : LSTM을 사용해 좌우 맥락을 모두 고려해 단어 하나를 예측하는 태스크를 통해 문맥적 표현을 학습하는 방법이 제안됨. 특정 단어를 가리고 맞추는 태스크가 문장 새성 모델의 강건함을 개선하는 데 도움이 됨을 보임. Unsupervised Fine-tuning Approaches 라벨링되지 않은 텍스트에서 단어 임베딩 파라미터만을 사전학습한 작업물이 시초. 최근은 문맥적 토큰 표현을 생성하는 단어, 문서 인코더가 라벨링되지 않은 문장으로부터 사전학습되고, 하위 태스크를 위해 파인튜닝됨. 완전히 처음부터 학습되어야 하는 파라미터의 수가 적다는 장점이 있음. 이 장점 덕분에, GPT는 GLUE 벤치마크를 통해 문장 단위 태스크에 있어 SOTA를 달성함. 좌-우 언어 모델링, 오토 인코더 목적이 이런 모델을 사전 훈련하는 데 있어 사용됨. Transfer Learning from Supervised Data 대규모 데이터셋을 사용한 지도 학습에서도 효과적인 전이가 가능하다는 것을 확인할 수 있었음. 컴퓨터 비전 연구에서도 이미지넷으로 사전 훈련된 모델을 파인튜닝하는 것이 효과적임을 확인함으로써, 큰 사전학습된 모델로부터 전이학습하는 것이 중요함을 확인할 수 있었음. BERT 모델 구조 original transformer를 기반으로 한 multi-layer bidirectional Transformer encoder를 사용. 두 가지의 모델을 사용. L : layer number, h : hidden size, a : number of self-attention heads BERT base : L=12, H=768, A=12, total parameters : 110M (GPT와 비교를 위해 같은 모델 사이즈를 사용) BERT large : L=24, H=1024, A=16, total parameters=340M 입/출력 표현 다양한 하위 태스크를 처리할 수 있도록 하기 위해 하나의 토큰 시퀀스에 하나의 문장과 문장 두 개를 모호하지 않게 인풋에 담을 수 있도록 함.\n","keywords":[],"articleBody":"Abstract Bidirectional Encoder Representations from Transformers (트랜스포머를 통한 양방향 인코더 표현)을 소개 기존 언어 표현 모델과 다르게, BERT는 모든 레이어의 왼쪽과 오른쪽 맥락을 동시에 고려하는 방식으로 라벨링되지 않은 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계. -\u003e 테스크를 위한 구체적인 구조 수정 없이도 하나의 아웃풋 레이어를 추가하는 것 만으로 다양한 태스크를 위한 SOTA 모델을 생성하는 것이 가능. Introduction 언어모델 사전 학습이 많은 NLP 태스크에 있어 효과적임이 증명됨. 문장 단계의 태스크 : 문장 간의 연관성을 전체적으로 분석함으로써 예측 자연 언어 추론 parapharsing 토큰 단계의 태스크 : 토큰 수준에서 세밀한 아웃풋을 생성 개체명 인식 질문 답변 사전학습된 언어 표현을 후속 태스크에 적용하기 위한 방법은 두 가지가 있음. feature-based (ex: ELMo) 추가적 특징으로 사전학습된 표현을 포함하는 태스크 구체적인 구조를 사용. fine-tuning (ex: OpenAI GPT) 최소한의 태스크 구체적인 파라미터를 도입 모든 사전학습된 파라미터를 파인튜닝함으로써 후속 태스크에 맞춰 학습. 이런 기존의 기술이 특히 fine-tuning 기법에서 사전 학습된 표현의 힘을 제약한다고 주장. 주요 제한 사항 : 현대 언어 모델은 단방향이라는 것. ex) GPT : 왼쪽에서 오른쪽으로 진행하는 구조를 사용 -\u003e 트랜스포머의 self-attention layer의 이전 토큰만을 접근 가능. -\u003e 이러한 제약 사항은 문장 수준에 태스크에 있어 최선은 아니고, 양쪽 방향에서의 맥락을 통합하는 것이 중요한 토큰 단위 태스크에서 fine-tuning 기법을 적용하는 것은 위험할 수 있음. 이 paper에서는 BERT를 제안함으로써 파인튜닝 기반 기법을 개선함. Masked Language Model을 통해 사전학습을 함으로써 단방향성의 제약을 완화. MLM : 인풋의 토큰을 랜덤으로 가린 뒤 문맥만을 통해 가려진 단어를 유추. 왼쪽과 오른쪽 문맥을 표현이 융합할 수 있도록 함. 다음 문장 예측 태스크를 통해 문장 쌍 표현을 연결해 사전훈련 기여는 다음과 같음. 언어 표현을 위한 양방향 사전 학습의 중요성을 설명. MLM을 통해 사전학습된 깊은 양방향 표현을 가능하도록 함. 이는 GPT : 사전학습을 위해 단방향 모델을 사용, 그리고 ELMo : 독립적으로 왼쪽-오른쪽, 오른쪽-왼쪽으로 학습된 언어모델을 얕게 연결하기만 함 과 다름. 사전학습된 표현이 태스크를 위한 구조를 설계하기 위한 노력을 줄인다는 것을 보임. 많은 태스크 구체적인 구조보다 더 잘 작동함으로써 BERT가 넓은 범위의 sentence level, token level의 task에 SOTA를 달성함을 보임. 7개의 NLP 태스크에서 SOTA를 보임. Related Work Unsupervised Feature-based Approaches non-neural과 neural에 걸쳐 널리 사용할 수 있는 단어의 표현을 학습하는 연구는 활발히 진행되어 왔음. 사전 훈련된 단어 임베딩 : 현대 NLP 시스템의 가장 필수적인 부분. 좌에서 우로 단어 예측, 또는 좌우 문백을 통해 올바른 단어와 틀린 단어를 구분하는 목표가 사용됨. 이러한 접근이 문장이나 문단 임베딩과 같은 더 덜 세밀한 범위로 일반화됨. 다음과 같은 목표들이 문장 표현 훈련을 위해 사용됨. 다음 문장 후보 순위를 매김. 이전 문장의 표현을 주고 다음 문장 단어를 좌에서 우로 생성 오토인코더로 생성된 목표의 노이즈를 제거 ELMo와 그 이전 연구들 : 단어 임베딩을 다른 차원에서 일반화함. 좌-\u003e우, 우-\u003e좌 LM을 통해 문맥과 연관된 특징을 추출. 각 토큰의 문맥적 표현은 좌-\u003e우, 우-\u003e좌 표현이 합쳐진 것. 존재하는 태스크 구체적인 구조와 문맥적인 단어 임베딩을 통합했을 때, 7개의 major NLP task에서 SOTA를 달성함. 그 외 : LSTM을 사용해 좌우 맥락을 모두 고려해 단어 하나를 예측하는 태스크를 통해 문맥적 표현을 학습하는 방법이 제안됨. 특정 단어를 가리고 맞추는 태스크가 문장 새성 모델의 강건함을 개선하는 데 도움이 됨을 보임. Unsupervised Fine-tuning Approaches 라벨링되지 않은 텍스트에서 단어 임베딩 파라미터만을 사전학습한 작업물이 시초. 최근은 문맥적 토큰 표현을 생성하는 단어, 문서 인코더가 라벨링되지 않은 문장으로부터 사전학습되고, 하위 태스크를 위해 파인튜닝됨. 완전히 처음부터 학습되어야 하는 파라미터의 수가 적다는 장점이 있음. 이 장점 덕분에, GPT는 GLUE 벤치마크를 통해 문장 단위 태스크에 있어 SOTA를 달성함. 좌-우 언어 모델링, 오토 인코더 목적이 이런 모델을 사전 훈련하는 데 있어 사용됨. Transfer Learning from Supervised Data 대규모 데이터셋을 사용한 지도 학습에서도 효과적인 전이가 가능하다는 것을 확인할 수 있었음. 컴퓨터 비전 연구에서도 이미지넷으로 사전 훈련된 모델을 파인튜닝하는 것이 효과적임을 확인함으로써, 큰 사전학습된 모델로부터 전이학습하는 것이 중요함을 확인할 수 있었음. BERT 모델 구조 original transformer를 기반으로 한 multi-layer bidirectional Transformer encoder를 사용. 두 가지의 모델을 사용. L : layer number, h : hidden size, a : number of self-attention heads BERT base : L=12, H=768, A=12, total parameters : 110M (GPT와 비교를 위해 같은 모델 사이즈를 사용) BERT large : L=24, H=1024, A=16, total parameters=340M 입/출력 표현 다양한 하위 태스크를 처리할 수 있도록 하기 위해 하나의 토큰 시퀀스에 하나의 문장과 문장 두 개를 모호하지 않게 인풋에 담을 수 있도록 함.\n문장이라는 것이 실제 언어적으로 문장이라는 의미보다, 연속된 텍스트의 무작위적인 범위로 생각.\n시퀀스 : BERT의 인풋 토큰 시퀀스. 한 문장이나 합쳐진 두 문장이 될 수 있음.\n30000개의 단어를 가진 WordPiece 임베딩을 사용.\n모든 시퀀스의 시작 : [CLS] 토큰\n이 토큰과 연관된 마지막 hidden state가 분류 task를 위한 전체 시퀀스의 대표 벡터로 사용됨. 문장 쌍은 시퀀스 하나에 들어가는데, 이는 두 방법으로 구별됨.\n[SEP] 토큰을 통해 문장을 구분. 어느 문장에 속하는지 모든 토큰에 임베딩을 추가. 인풋 표현은 해당하는 토큰 임베딩과 어느 문장에 속하는지 임베딩, 그리고 위치 임베딩을 더해 구해짐.\nBERT 사전 학습 두 가지의 비지도 task를 통해 사전 학습됨. Masked LM 양방향 상황은 단어가 서로를 확인해서, 모델이 다층 맥락에서 타겟 단어를 너무 쉽게 예측할 수 있는 문제점이 존재. 이 연구에서는 인풋 토큰의 일부를 가린 뒤, 해당 가려진 토큰을 예측하는 방식으로 훈련. 가려진 토큰에 해당하는 퇴종 hidden vector를 소프트맥스에 넣어 단어를 예측함. 파인튜닝 시에는 [MASK] 토큰이 사용되지 않음 -\u003e 항상 해당 토큰을 사용하지 않고, 랜덤 토큰이나 원래 그대로의 토큰도 사용. Next Sentence Prediction QA, NLI와 같은 task는 두 문장 간의 관계를 이해하는 것이 필요. -\u003e 이진화된 다음 문장 예측 task를 사전 학습. 두 문장을 고르고, 50퍼는 이어지는 문장, 50퍼는 이어지지 않는 문장. C 벡터가 NSP에 사용됨. 간단하지만, QA와 NLI task에 모두 아주 도움이 됨. 기존 연구와 유사하지만, 오로지 문장 임베딩만이 전이되는 기존 연구와는 다른게 BERT는 모든 파라미터가 전이됨. Pre-training data 언어 모델 사전 학습의 기존 문헌을 따라감. BooksCorpus, Wikipedia를 사용. 긴 연속된 시퀀스를 추출하기 위해 문서 단위 코퍼스를 사용하는 것이 중요. BERT 파인 튜닝 transformer의 self-attention : 알맞은 input과 output을 넣는 것만으로도 많은 하위 task를 수행할 수 있도록 함. 문장 쌍을 사용하는 응용 : 보통 양방향 cross attention 이전에 문장을 각각 인코딩 BERT : 두 단계를 합치기 위해 self-attention mechanism 사용 - 연결된 문장 쌍을 self-attention하는 것이 두 문장 간의 양방향 cross attention을 더 효과적으로 포함하기 때문. 모든 태스크에 대해 해당하는 Input과 output을 넣고 모든 파라미터를 파인튜닝함. input은 다음과 같은 관계를 가짐. paraphrasing을 위한 문장 쌍 가설-전제 쌍 질문-문서 쌍 문장 하나 토큰 표현 : 토큰 수준 task에 사용됨. [CLS] 표현 : 분류에 사용됨. fine-tuning : 비교적으로 비용이 덜 듬. Experiments GLUE General Language Understanding Evaluation section 3에서 언급된 것처럼 문장을 정의한 뒤 [CLS] 토큰의 마지막 hidden vector에 해당하는 C를 대표 표현으로 사용. 파인튜닝 과정에서 새롭게 도입된 파라미터는 분류 레이어의 weight W뿐 일반 분류 손실을 log(softmax(CW ^T))로 계산함. ![[스크린샷 2025-03-12 오후 4.38.15.png]] batch size : 32, 3 epochs task에 따라 학습률을 다르게 함. 단, BERT large 모델은 작은 데이터셋에서 파인튜닝이 불안정하다는 것을 발견 -\u003e random restart 적용 : 같은 훈련 지점에서 데이터 shuffling과 분류기 층 초기화를 진행. 모든 테스크에서 SOTA를 달성 BERT base와 GPT는 attention masking을 제외하고는 거의 같은 구조를 가짐. BERT large가 BERT base보다 높은 성능을 보임, 특히 훈련 데이터가 적은 곳에서 그런 결과를 보임. SQuAD v1.1 질문과 답을 포함하고 있는 위키피디아 문서를 주면, 정답 텍스트 부분을 찾아내는 task 파인 튜닝 과정에서 시작 벡터 S와 끝 벡터 E만을 추가. 전체 단어에 대해 소프트맥스를 수행해 해당 단어가 답이 될 가능성을 계산. $- $ ","wordCount":"1135","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/bert-%EC%A0%95%EB%A6%AC/"},"publisher":{"@type":"Organization","name":"CS Playground","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=archives><span>archives</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent"></h1><div class=post-meta><span>6 min</span>&nbsp;·&nbsp;<span>Me</span>&nbsp;|&nbsp;<span>
<a href=https://github.com/%3cpath_to_repo%3e/content/posts/BERT%20%ec%a0%95%eb%a6%ac.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#related-work>Related Work</a><ul><li><a href=#unsupervised-feature-based-approaches>Unsupervised Feature-based Approaches</a></li><li><a href=#unsupervised-fine-tuning-approaches>Unsupervised Fine-tuning Approaches</a></li><li><a href=#transfer-learning-from-supervised-data>Transfer Learning from Supervised Data</a></li></ul></li><li><a href=#bert>BERT</a><ul><li><a href=#모델-구조>모델 구조</a></li><li><a href=#입출력-표현>입/출력 표현</a></li><li><a href=#bert-사전-학습>BERT 사전 학습</a></li><li><a href=#bert-파인-튜닝>BERT 파인 튜닝</a></li></ul></li><li><a href=#experiments>Experiments</a><ul><li><a href=#glue>GLUE</a></li><li><a href=#squad-v11>SQuAD v1.1</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><ul><li>Bidirectional Encoder Representations from Transformers (트랜스포머를 통한 양방향 인코더 표현)을 소개</li><li>기존 언어 표현 모델과 다르게, BERT는 모든 레이어의 왼쪽과 오른쪽 맥락을 동시에 고려하는 방식으로 라벨링되지 않은 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계.<ul><li>-> 테스크를 위한 구체적인 구조 수정 없이도 하나의 아웃풋 레이어를 추가하는 것 만으로 다양한 태스크를 위한 SOTA 모델을 생성하는 것이 가능.</li></ul></li></ul><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><ul><li>언어모델 사전 학습이 많은 NLP 태스크에 있어 효과적임이 증명됨.<ul><li>문장 단계의 태스크 : 문장 간의 연관성을 전체적으로 분석함으로써 예측<ul><li>자연 언어 추론</li><li>parapharsing</li></ul></li><li>토큰 단계의 태스크 : 토큰 수준에서 세밀한 아웃풋을 생성<ul><li>개체명 인식</li><li>질문 답변</li></ul></li></ul></li><li>사전학습된 언어 표현을 후속 태스크에 적용하기 위한 방법은 두 가지가 있음.<ul><li>feature-based (ex: ELMo)<ul><li>추가적 특징으로 사전학습된 표현을 포함하는 태스크 구체적인 구조를 사용.</li></ul></li><li>fine-tuning (ex: OpenAI GPT)<ul><li>최소한의 태스크 구체적인 파라미터를 도입</li><li>모든 사전학습된 파라미터를 파인튜닝함으로써 후속 태스크에 맞춰 학습.</li></ul></li></ul></li><li>이런 기존의 기술이 특히 fine-tuning 기법에서 사전 학습된 표현의 힘을 제약한다고 주장.<ul><li>주요 제한 사항 : 현대 언어 모델은 <strong>단방향</strong>이라는 것.</li><li>ex) GPT : 왼쪽에서 오른쪽으로 진행하는 구조를 사용 -> 트랜스포머의 self-attention layer의 이전 토큰만을 접근 가능.</li><li>-> 이러한 제약 사항은 문장 수준에 태스크에 있어 최선은 아니고, 양쪽 방향에서의 맥락을 통합하는 것이 중요한 토큰 단위 태스크에서 fine-tuning 기법을 적용하는 것은 위험할 수 있음.</li></ul></li><li>이 paper에서는 BERT를 제안함으로써 파인튜닝 기반 기법을 개선함.<ul><li><strong>Masked Language Model</strong>을 통해 사전학습을 함으로써 단방향성의 제약을 완화.<ul><li>MLM : 인풋의 토큰을 랜덤으로 가린 뒤 문맥만을 통해 가려진 단어를 유추.</li><li>왼쪽과 오른쪽 문맥을 표현이 융합할 수 있도록 함.</li></ul></li><li><strong>다음 문장 예측 태스크</strong>를 통해 문장 쌍 표현을 연결해 사전훈련</li></ul></li><li>기여는 다음과 같음.<ul><li>언어 표현을 위한 양방향 사전 학습의 중요성을 설명.<ul><li>MLM을 통해 사전학습된 깊은 양방향 표현을 가능하도록 함.</li><li>이는 GPT : 사전학습을 위해 단방향 모델을 사용, 그리고 ELMo : 독립적으로 왼쪽-오른쪽, 오른쪽-왼쪽으로 학습된 언어모델을 얕게 연결하기만 함 과 다름.</li></ul></li><li>사전학습된 표현이 태스크를 위한 구조를 설계하기 위한 노력을 줄인다는 것을 보임.<ul><li>많은 태스크 구체적인 구조보다 더 잘 작동함으로써 BERT가 넓은 범위의 sentence level, token level의 task에 SOTA를 달성함을 보임.</li></ul></li><li>7개의 NLP 태스크에서 SOTA를 보임.</li></ul></li></ul><h2 id=related-work>Related Work<a hidden class=anchor aria-hidden=true href=#related-work>#</a></h2><h3 id=unsupervised-feature-based-approaches>Unsupervised Feature-based Approaches<a hidden class=anchor aria-hidden=true href=#unsupervised-feature-based-approaches>#</a></h3><ul><li>non-neural과 neural에 걸쳐 널리 사용할 수 있는 단어의 표현을 학습하는 연구는 활발히 진행되어 왔음.</li><li>사전 훈련된 단어 임베딩 : 현대 NLP 시스템의 가장 필수적인 부분.<ul><li>좌에서 우로 단어 예측, 또는 좌우 문백을 통해 올바른 단어와 틀린 단어를 구분하는 목표가 사용됨.</li></ul></li><li>이러한 접근이 문장이나 문단 임베딩과 같은 더 덜 세밀한 범위로 일반화됨.<ul><li>다음과 같은 목표들이 문장 표현 훈련을 위해 사용됨.<ul><li>다음 문장 후보 순위를 매김.</li><li>이전 문장의 표현을 주고 다음 문장 단어를 좌에서 우로 생성</li><li>오토인코더로 생성된 목표의 노이즈를 제거</li></ul></li></ul></li><li>ELMo와 그 이전 연구들 : 단어 임베딩을 다른 차원에서 일반화함.<ul><li>좌->우, 우->좌 LM을 통해 문맥과 연관된 특징을 추출.</li><li>각 토큰의 문맥적 표현은 좌->우, 우->좌 표현이 합쳐진 것.</li><li>존재하는 태스크 구체적인 구조와 문맥적인 단어 임베딩을 통합했을 때, 7개의 major NLP task에서 SOTA를 달성함.</li><li>그 외 : LSTM을 사용해 좌우 맥락을 모두 고려해 단어 하나를 예측하는 태스크를 통해 문맥적 표현을 학습하는 방법이 제안됨. 특정 단어를 가리고 맞추는 태스크가 문장 새성 모델의 강건함을 개선하는 데 도움이 됨을 보임.</li></ul></li></ul><h3 id=unsupervised-fine-tuning-approaches>Unsupervised Fine-tuning Approaches<a hidden class=anchor aria-hidden=true href=#unsupervised-fine-tuning-approaches>#</a></h3><ul><li>라벨링되지 않은 텍스트에서 단어 임베딩 파라미터만을 사전학습한 작업물이 시초.</li><li>최근은 문맥적 토큰 표현을 생성하는 단어, 문서 인코더가 라벨링되지 않은 문장으로부터 사전학습되고, 하위 태스크를 위해 파인튜닝됨.<ul><li>완전히 처음부터 학습되어야 하는 파라미터의 수가 적다는 장점이 있음.</li></ul></li><li>이 장점 덕분에, GPT는 GLUE 벤치마크를 통해 문장 단위 태스크에 있어 SOTA를 달성함.</li><li>좌-우 언어 모델링, 오토 인코더 목적이 이런 모델을 사전 훈련하는 데 있어 사용됨.</li></ul><h3 id=transfer-learning-from-supervised-data>Transfer Learning from Supervised Data<a hidden class=anchor aria-hidden=true href=#transfer-learning-from-supervised-data>#</a></h3><ul><li>대규모 데이터셋을 사용한 지도 학습에서도 효과적인 전이가 가능하다는 것을 확인할 수 있었음.</li><li>컴퓨터 비전 연구에서도 이미지넷으로 사전 훈련된 모델을 파인튜닝하는 것이 효과적임을 확인함으로써, 큰 사전학습된 모델로부터 전이학습하는 것이 중요함을 확인할 수 있었음.</li></ul><h2 id=bert>BERT<a hidden class=anchor aria-hidden=true href=#bert>#</a></h2><h3 id=모델-구조>모델 구조<a hidden class=anchor aria-hidden=true href=#모델-구조>#</a></h3><ul><li>original transformer를 기반으로 한 multi-layer bidirectional Transformer encoder를 사용.</li><li>두 가지의 모델을 사용. L : layer number, h : hidden size, a : number of self-attention heads</li><li>BERT base : L=12, H=768, A=12, total parameters : 110M (GPT와 비교를 위해 같은 모델 사이즈를 사용)</li><li>BERT large : L=24, H=1024, A=16, total parameters=340M</li></ul><h3 id=입출력-표현>입/출력 표현<a hidden class=anchor aria-hidden=true href=#입출력-표현>#</a></h3><ul><li><p>다양한 하위 태스크를 처리할 수 있도록 하기 위해 하나의 토큰 시퀀스에 하나의 문장과 문장 두 개를 모호하지 않게 인풋에 담을 수 있도록 함.</p></li><li><p>문장이라는 것이 실제 언어적으로 문장이라는 의미보다, 연속된 텍스트의 무작위적인 범위로 생각.</p></li><li><p>시퀀스 : BERT의 인풋 토큰 시퀀스. 한 문장이나 합쳐진 두 문장이 될 수 있음.</p></li><li><p>30000개의 단어를 가진 WordPiece 임베딩을 사용.</p></li><li><p>모든 시퀀스의 시작 : [CLS] 토큰</p><ul><li>이 토큰과 연관된 마지막 hidden state가 분류 task를 위한 전체 시퀀스의 대표 벡터로 사용됨.</li></ul></li><li><p>문장 쌍은 시퀀스 하나에 들어가는데, 이는 두 방법으로 구별됨.</p><ul><li>[SEP] 토큰을 통해 문장을 구분.</li><li>어느 문장에 속하는지 모든 토큰에 임베딩을 추가.</li></ul></li><li><p>인풋 표현은 해당하는 토큰 임베딩과 어느 문장에 속하는지 임베딩, 그리고 위치 임베딩을 더해 구해짐.</p></li></ul><h3 id=bert-사전-학습>BERT 사전 학습<a hidden class=anchor aria-hidden=true href=#bert-사전-학습>#</a></h3><ul><li>두 가지의 비지도 task를 통해 사전 학습됨.</li></ul><h4 id=masked-lm>Masked LM<a hidden class=anchor aria-hidden=true href=#masked-lm>#</a></h4><ul><li>양방향 상황은 단어가 서로를 확인해서, 모델이 다층 맥락에서 타겟 단어를 너무 쉽게 예측할 수 있는 문제점이 존재.</li><li>이 연구에서는 인풋 토큰의 일부를 가린 뒤, 해당 가려진 토큰을 예측하는 방식으로 훈련.</li><li>가려진 토큰에 해당하는 퇴종 hidden vector를 소프트맥스에 넣어 단어를 예측함.</li><li>파인튜닝 시에는 [MASK] 토큰이 사용되지 않음 -> 항상 해당 토큰을 사용하지 않고, 랜덤 토큰이나 원래 그대로의 토큰도 사용.</li></ul><h4 id=next-sentence-prediction>Next Sentence Prediction<a hidden class=anchor aria-hidden=true href=#next-sentence-prediction>#</a></h4><ul><li>QA, NLI와 같은 task는 두 문장 간의 관계를 이해하는 것이 필요. -> 이진화된 다음 문장 예측 task를 사전 학습.</li><li>두 문장을 고르고, 50퍼는 이어지는 문장, 50퍼는 이어지지 않는 문장.</li><li>C 벡터가 NSP에 사용됨.</li><li>간단하지만, QA와 NLI task에 모두 아주 도움이 됨.</li><li>기존 연구와 유사하지만, 오로지 문장 임베딩만이 전이되는 기존 연구와는 다른게 BERT는 모든 파라미터가 전이됨.</li></ul><h4 id=pre-training-data>Pre-training data<a hidden class=anchor aria-hidden=true href=#pre-training-data>#</a></h4><ul><li>언어 모델 사전 학습의 기존 문헌을 따라감.</li><li>BooksCorpus, Wikipedia를 사용.</li><li>긴 연속된 시퀀스를 추출하기 위해 문서 단위 코퍼스를 사용하는 것이 중요.</li></ul><h3 id=bert-파인-튜닝>BERT 파인 튜닝<a hidden class=anchor aria-hidden=true href=#bert-파인-튜닝>#</a></h3><ul><li>transformer의 self-attention : 알맞은 input과 output을 넣는 것만으로도 많은 하위 task를 수행할 수 있도록 함.</li><li>문장 쌍을 사용하는 응용 : 보통 양방향 cross attention 이전에 문장을 각각 인코딩</li><li>BERT : 두 단계를 합치기 위해 self-attention mechanism 사용 - 연결된 문장 쌍을 self-attention하는 것이 두 문장 간의 양방향 cross attention을 더 효과적으로 포함하기 때문.</li><li>모든 태스크에 대해 해당하는 Input과 output을 넣고 모든 파라미터를 파인튜닝함.</li><li>input은 다음과 같은 관계를 가짐.<ul><li>paraphrasing을 위한 문장 쌍</li><li>가설-전제 쌍</li><li>질문-문서 쌍</li><li>문장 하나</li></ul></li><li>토큰 표현 : 토큰 수준 task에 사용됨.</li><li>[CLS] 표현 : 분류에 사용됨.</li><li>fine-tuning : 비교적으로 비용이 덜 듬.</li></ul><h2 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h2><h3 id=glue>GLUE<a hidden class=anchor aria-hidden=true href=#glue>#</a></h3><ul><li>General Language Understanding Evaluation</li><li>section 3에서 언급된 것처럼 문장을 정의한 뒤 [CLS] 토큰의 마지막 hidden vector에 해당하는 C를 대표 표현으로 사용.</li><li>파인튜닝 과정에서 새롭게 도입된 파라미터는 분류 레이어의 weight W뿐</li><li>일반 분류 손실을 log(softmax(CW ^T))로 계산함.</li><li>![[스크린샷 2025-03-12 오후 4.38.15.png]]</li><li>batch size : 32, 3 epochs</li><li>task에 따라 학습률을 다르게 함.<ul><li>단, BERT large 모델은 작은 데이터셋에서 파인튜닝이 불안정하다는 것을 발견 -> random restart 적용 : 같은 훈련 지점에서 데이터 shuffling과 분류기 층 초기화를 진행.</li></ul></li><li>모든 테스크에서 SOTA를 달성</li><li>BERT base와 GPT는 attention masking을 제외하고는 거의 같은 구조를 가짐.</li><li>BERT large가 BERT base보다 높은 성능을 보임, 특히 훈련 데이터가 적은 곳에서 그런 결과를 보임.</li></ul><h3 id=squad-v11>SQuAD v1.1<a hidden class=anchor aria-hidden=true href=#squad-v11>#</a></h3><ul><li>질문과 답을 포함하고 있는 위키피디아 문서를 주면, 정답 텍스트 부분을 찾아내는 task</li><li>파인 튜닝 과정에서 시작 벡터 S와 끝 벡터 E만을 추가.</li><li>전체 단어에 대해 소프트맥스를 수행해 해당 단어가 답이 될 가능성을 계산.
$- $</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/boj_5549/><span class=title>« Prev</span><br><span>BOJ 5549 - 행성 탐사(C++)</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share  on x" href="https://x.com/intent/tweet/?text=&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fbert-%25EC%25A0%2595%25EB%25A6%25AC%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share  on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fbert-%25EC%25A0%2595%25EB%25A6%25AC%2f&amp;title=&amp;summary=&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fbert-%25EC%25A0%2595%25EB%25A6%25AC%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share  on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fbert-%25EC%25A0%2595%25EB%25A6%25AC%2f&title="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share  on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fbert-%25EC%25A0%2595%25EB%25A6%25AC%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share  on whatsapp" href="https://api.whatsapp.com/send?text=%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fbert-%25EC%25A0%2595%25EB%25A6%25AC%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share  on telegram" href="https://telegram.me/share/url?text=&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fbert-%25EC%25A0%2595%25EB%25A6%25AC%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share  on ycombinator" href="https://news.ycombinator.com/submitlink?t=&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fbert-%25EC%25A0%2595%25EB%25A6%25AC%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>CS Playground</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>